{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da158408-49ff-45b8-8588-fed79a55b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcffd3b0-bdf1-400b-a2d0-ac250cb94f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# PyTorch and torchvision\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95df9526-edf0-4359-8246-e6c9c4d52769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU 확인\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63863b7-a65b-4410-9303-aeda047d56af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-22 06:35:33--  http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n",
      "Resolving vision.stanford.edu (vision.stanford.edu)... 171.64.68.10\n",
      "Connecting to vision.stanford.edu (vision.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 793579520 (757M) [application/x-tar]\n",
      "Saving to: ‘/home/jovyan/work/data_augmentation/data/images.tar.2’\n",
      "\n",
      "images.tar.2          6%[>                   ]  46.08M  3.15MB/s    eta 3m 57s "
     ]
    }
   ],
   "source": [
    "!wget \"http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\" -P ~/work/data_augmentation/data\n",
    "!tar -xf ~/work/data_augmentation/data/images.tar -C ~/work/data_augmentation/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef2722-5ebd-4d10-b2bc-ce84c7417d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "dataset_dir = \"~/work/data_augmentation/data/Images/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 크기 통일\n",
    "    transforms.ToTensor(),  # Tensor 변환\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # 정규화 추가\n",
    "])\n",
    "full_dataset = ImageFolder(root=dataset_dir, transform=transform)\n",
    "\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.583 * total_size)  # 약 12,000개\n",
    "test_size = total_size - train_size   # 약 8,580개\n",
    "ds_train, ds_test = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(ds_train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(ds_test, batch_size=32, shuffle=False)\n",
    "ds_info = {\n",
    "    \"num_classes\": len(full_dataset.classes),\n",
    "    \"class_names\": full_dataset.classes\n",
    "}\n",
    "\n",
    "print(\"=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdc310b-8646-44fe-b45c-f7bdb79e00ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader에서 일부 배치 가져오기\n",
    "def show_examples(data_loader, class_names, num_images=6):\n",
    "    data_iter = iter(data_loader)\n",
    "    images, labels = next(data_iter)\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "    for i in range(num_images):\n",
    "        image = images[i].permute(1, 2, 0).numpy()  # (C, H, W) → (H, W, C)\n",
    "        image = (image * 0.5) + 0.5\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(class_names[labels[i].item()])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 훈련 데이터 샘플 시각화\n",
    "show_examples(train_loader, ds_info[\"class_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad85035a-be07-4968-a9e2-f9e5c85ce43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_resize_img():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # 크기 통일\n",
    "        transforms.ToTensor(),  # Tensor 변환\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # 정규화 추가\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea22e90-2957-41ed-ade4-cdad80ce9279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment():\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.2)\n",
    "    ])\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ed0b05-630f-4ef9-893c-a9220cc2da0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 가공하는 메인함수\n",
    "def apply_normalize_on_dataset(dataset, is_test=False, batch_size=16, with_aug=False):\n",
    "    transform = normalize_and_resize_img()\n",
    "\n",
    "    if not is_test and with_aug:\n",
    "        dataset.dataset.transform = transforms.Compose([\n",
    "            *augment().transforms,\n",
    "            *transform.transforms\n",
    "        ])\n",
    "    else:\n",
    "        dataset.dataset.transform = transform\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=not is_test, num_workers=2, pin_memory=True)\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e6696-8261-4727-bcb9-0153710b3858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def augment2():\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # 좌우 반전\n",
    "        transforms.RandomVerticalFlip(p=0.5),    # 상하 반전\n",
    "        transforms.RandomRotation(degrees=(0, 90, 180, 270)),  # 90도 단위 회전\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # 밝기, 대비, 색상 조정\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # 랜덤 크롭 후 리사이즈\n",
    "        transforms.Lambda(lambda img: torch.clamp(img, 0, 1))  # 0~1 값으로 클리핑\n",
    "    ])\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8fcaf-748a-4e06-a390-9e81128c6638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "num_classes = len(ds_info[\"class_names\"])\n",
    "resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "resnet50.fc = nn.Linear(resnet50.fc.in_features, num_classes)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    resnet50,\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(resnet50.fc.in_features, num_classes),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "print(\"=3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205bb384-029f-484e-9f62-b653cae50909",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "aug_resnet50.fc = nn.Linear(aug_resnet50.fc.in_features, num_classes)\n",
    "\n",
    "aug_resnet50 = nn.Sequential(\n",
    "    aug_resnet50,\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "print(\"=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d2ce9-71d2-4c50-bbc2-8bb731f5b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_no_aug = apply_normalize_on_dataset(ds_train, with_aug=False)\n",
    "ds_train_aug = apply_normalize_on_dataset(ds_train, with_aug=True)\n",
    "ds_test = apply_normalize_on_dataset(ds_test, is_test=True)\n",
    "\n",
    "print(\"=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207685ed-d168-4338-b2af-c04410a1265a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef533ab-6c1c-48db-9990-b81e2a0214d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954aa25-05ca-410e-a39d-b93915612a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f864d6-135d-4502-8ce7-746192426cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf27f558-1689-46ac-9916-a8e8ec2d58ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560ccd2c-1db7-451e-b30f-a7e43b3b16e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b637f6-845c-4947-adc0-d223b65138e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b12851-62cb-4b22-b13e-d0af9b7d6892",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch.optim as optim\n",
    "\n",
    "#EPOCH = 20  # Augentation 적용 효과를 확인하기 위해 필요한 epoch 수\n",
    "EPOCH = 3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet50.parameters(), lr=0.001)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# (참고용) 이전에 정의했던 함수들이 필요합니다.\n",
    "# def mixup_data(...): ...\n",
    "# def cutmix_data(...): ...\n",
    "# def mixup_criterion(...): ...\n",
    "\n",
    "def train(model, train_loader, test_loader, epochs, aug_method='none'):\n",
    "    model.to(device)\n",
    "    history = {'val_accuracy': []}\n",
    "    \n",
    "    print(f\"Start training with method: {aug_method}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # --- [핵심 수정 부분] Augmentation 적용 로직 ---\n",
    "            if aug_method == 'mixup':\n",
    "                # 1. MixUp 데이터 생성\n",
    "                images, targets_a, targets_b, lam = mixup_data(images, labels, alpha=1.0)\n",
    "                # 2. 모델 예측\n",
    "                outputs = model(images)\n",
    "                # 3. MixUp Loss 계산\n",
    "                loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "                \n",
    "            elif aug_method == 'cutmix':\n",
    "                # CutMix는 확률적으로(예: 50%) 적용하거나 항상 적용할 수 있습니다.\n",
    "                # 여기서는 항상 적용하는 것으로 작성합니다.\n",
    "                images, targets_a, targets_b, lam = cutmix_data(images, labels, alpha=1.0)\n",
    "                outputs = model(images)\n",
    "                loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "                \n",
    "            else:\n",
    "                # 일반 학습 (Basic)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            # ---------------------------------------------\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 정확도 계산 (MixUp/CutMix일 때는 원본 라벨 중 더 큰 비중을 가진 쪽과 비교하거나, 단순히 가장 높은 확률의 클래스로 계산)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_acc = 100. * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Method: {aug_method}, Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "        # --- 검증 (Validation) ---\n",
    "        # 검증 데이터에는 MixUp/CutMix를 적용하지 않습니다 (정석)\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "        print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "# --- 실행 예시 ---\n",
    "# 1. MixUp으로 학습\n",
    "# history_mixup = train(resnet50, train_loader, test_loader, EPOCH, aug_method='mixup')\n",
    "\n",
    "# 2. CutMix로 학습\n",
    "# history_cutmix = train(resnet50, train_loader, test_loader, EPOCH, aug_method='cutmix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893bc38-fd76-4808-a1e5-ec34e711a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(images, labels, batch_size=16, img_size=224, num_classes=120):\n",
    "    mixed_imgs = []\n",
    "    mixed_labels = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        image_a = images[i]\n",
    "        label_a = labels[i]\n",
    "\n",
    "        # 랜덤하게 다른 이미지 선택\n",
    "        j = torch.randint(0, batch_size, (1,)).item()\n",
    "        image_b = images[j]\n",
    "        label_b = labels[j]\n",
    "\n",
    "        # Mixup 적용\n",
    "        mixed_img, mixed_label = mixup_2_images(image_a, image_b, label_a, label_b, num_classes)\n",
    "\n",
    "        mixed_imgs.append(mixed_img)\n",
    "        mixed_labels.append(mixed_label)\n",
    "\n",
    "    # 배치 차원 추가\n",
    "    mixed_imgs = torch.stack(mixed_imgs).view(batch_size, 3, img_size, img_size)  # (B, C, H, W)\n",
    "    mixed_labels = torch.stack(mixed_labels).view(batch_size, num_classes)  # (B, num_classes)\n",
    "\n",
    "    return mixed_imgs, mixed_labels\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3da2a8d-b540-4f1d-b199-dc9deaa0f5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "history_mixup = train(resnet50, train_loader, test_loader, EPOCH, aug_method='mixup')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d5c8d-ce5a-4bc6-8f45-6e6206763ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_cutmix = train(resnet50, train_loader, test_loader, EPOCH, aug_method='cutmix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f9946a-27ca-4cd8-af98-250f4523087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_resnet50_no_aug['val_accuracy'], 'r', label='No Augmentation')\n",
    "plt.plot(history_resnet50_aug['val_accuracy'], 'b', label='With Augmentation')\n",
    "plt.title('Model validation accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dac1be9-bf07-442e-9006-cae94e809700",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_resnet50_no_aug['val_accuracy'], 'r', label='No Augmentation')\n",
    "plt.plot(history_resnet50_aug['val_accuracy'], 'b', label='With Augmentation')\n",
    "plt.title('Model validation accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.ylim(0.50, 0.80)    # 출력하고자 하는  Accuracy 범위를 지정해 주세요.\n",
    "#plt.ylim(0.72, 0.76)  # EPOCH=20으로 진행한다면 이 범위가 적당합니다.\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
